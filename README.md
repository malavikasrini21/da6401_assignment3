# Attention vs Vanilla Encoder-Decoder Comparison

This project compares two sequence-to-sequence transliteration systems:

Folder Structures:

- `attn_encoder_decoder`: Implements an encoder-decoder architecture **with attention**.
- `vanilla_encoder_decoder`: Implements a **vanilla encoder-decoder** model (no attention).

Each folder contains modular components for data loading, model training, evaluation, and prediction saving.

Use these implementations to compare the effectiveness of attention mechanisms in character-level transliteration (e.g., Latin to Devanagari).
