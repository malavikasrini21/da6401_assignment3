# Attention vs Vanilla Encoder-Decoder Comparison

This project compares two sequence-to-sequence transliteration systems evaluated on Dakshina dataset:

Folder Structures:

- `attn_encoder_decoder`: Implements an encoder-decoder architecture **with attention**.
- `vanilla_encoder_decoder`: Implements a **vanilla encoder-decoder** model (no attention).

Each folder contains modular components for data loading, model architecture designing,training, evaluation, and predictions saving.

Use these implementations to compare the effectiveness of attention mechanisms in character-level transliteration (e.g., Latin to Devanagari).

# wandb report link : https://wandb.ai/malavika_da24s010-indian-institute-of-technology-madras/da6401_assignment3/reports/DA6401-Assignment-3--VmlldzoxMjg1NTQwNg
